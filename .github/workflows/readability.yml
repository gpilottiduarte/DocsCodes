name: Documentation Readability Analysis
on:
  push:
    branches: [ main, master ]
    paths:
      - '**/*.md'
      - '**/*.mdx'
  pull_request:
    branches: [ main, master ]
    paths:
      - '**/*.md'
      - '**/*.mdx'
  workflow_dispatch:

env:
  READING_SPEED_WPM: 200

jobs:
  analyze-readability:
    name: Analyze Content Readability
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Create requirements file
        run: |
          echo "textstat>=0.7.3" > requirements.txt
          echo "pandas>=2.0.0" >> requirements.txt

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create readability analyzer script
        run: |
          cat > analyze_readability.py << 'EOF'
          import textstat
          import glob
          import json
          import os
          import sys
          from datetime import datetime
          from typing import Dict, List, Optional

          class ReadabilityAnalyzer:
              def __init__(self, config: Dict):
                  self.reading_speed_wpm = float(config.get('READING_SPEED_WPM', 200))
                  self.ignore_files = {
                      'CHANGELOG.md', 
                      'LICENSE.md',
                      'CODE_OF_CONDUCT.md',
                      'CONTRIBUTING.md'
                  }
                  self.results = []

              def analyze_file(self, filepath: str) -> Optional[Dict]:
                  """Analyze a single file for readability metrics without judgement."""
                  try:
                      with open(filepath, 'r', encoding='utf-8') as f:
                          content = f.read()
                          
                      # Collect all metrics
                      words = textstat.lexicon_count(content, removepunct=True)
                      sentences = textstat.sentence_count(content)
                      syllables = textstat.syllable_count(content)
                      
                      return {
                          'filepath': filepath,
                          'metrics': {
                              'flesch_reading_ease': textstat.flesch_reading_ease(content),
                              'flesch_kincaid_grade': textstat.flesch_kincaid_grade(content),
                              'gunning_fog_index': textstat.gunning_fog(content),
                              'smog_index': textstat.smog_index(content),
                              'coleman_liau_index': textstat.coleman_liau_index(content),
                              'automated_readability_index': textstat.automated_readability_index(content),
                              'dale_chall_readability_score': textstat.dale_chall_readability_score(content)
                          },
                          'statistics': {
                              'word_count': words,
                              'sentence_count': sentences,
                              'syllable_count': syllables,
                              'avg_words_per_sentence': words / sentences if sentences > 0 else 0,
                              'avg_syllables_per_word': syllables / words if words > 0 else 0,
                              'estimated_reading_time_minutes': (words / self.reading_speed_wpm)
                          }
                      }
                  except Exception as e:
                      print(f'Error analyzing {filepath}: {str(e)}')
                      return None

              def analyze_documentation(self) -> Dict:
                  """Analyze all markdown files and generate a report."""
                  # Find all markdown files
                  markdown_files = glob.glob('**/*.md', recursive=True)
                  if not markdown_files:
                      print("No markdown files found to analyze!")
                      return {'summary': {'total_files': 0}}
                  
                  for filepath in markdown_files:
                      if os.path.basename(filepath) not in self.ignore_files:
                          result = self.analyze_file(filepath)
                          if result:
                              self.results.append(result)
                  
                  # Generate report
                  report = {
                      'timestamp': datetime.now().isoformat(),
                      'summary': {
                          'total_files': len(self.results),
                          'configuration': {
                              'reading_speed_wpm': self.reading_speed_wpm
                          }
                      },
                      'results': self.results
                  }
                  
                  # Ensure directory exists
                  os.makedirs('readability_report', exist_ok=True)
                  
                  # Save detailed report
                  with open('readability_report/detailed_report.json', 'w') as f:
                      json.dump(report, f, indent=2)
                  
                  return report

          def main():
              config = {
                  'READING_SPEED_WPM': os.environ.get('READING_SPEED_WPM', '200')
              }
              
              analyzer = ReadabilityAnalyzer(config)
              report = analyzer.analyze_documentation()
              
              # Print summary to console
              print('\nüìä Documentation Readability Analysis')
              print('===================================')
              print(f"\nAnalyzed {report['summary']['total_files']} files")
              
              # Print detailed results
              if report['results']:
                  for result in report['results']:
                      print(f"\nüìÑ {result['filepath']}")
                      print("Readability Metrics:")
                      for metric, value in result['metrics'].items():
                          print(f"  - {metric}: {value:.2f}")
                      print("\nDocument Statistics:")
                      for stat, value in result['statistics'].items():
                          if 'time' in stat:
                              print(f"  - {stat}: {value:.1f} minutes")
                          else:
                              print(f"  - {stat}: {value:.2f}")

          if __name__ == '__main__':
              main()
          EOF

      - name: Run readability analysis
        run: python analyze_readability.py

      - name: Upload report
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: readability-report
          path: readability_report/

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            try {
              const report = JSON.parse(fs.readFileSync('readability_report/detailed_report.json', 'utf8'));
              
              let comment = '## üìä Documentation Readability Analysis\n\n';
              
              comment += `### Summary\n`;
              comment += `- Total files analyzed: ${report.summary.total_files}\n`;
              comment += `- Reading speed: ${report.summary.configuration.reading_speed_wpm} words per minute\n\n`;
              
              if (report.results && report.results.length > 0) {
                for (const result of report.results) {
                  comment += `### üìÑ ${result.filepath}\n\n`;
                  
                  comment += '#### Readability Metrics\n';
                  comment += '| Metric | Score |\n';
                  comment += '|--------|-------|\n';
                  for (const [metric, value] of Object.entries(result.metrics)) {
                    comment += `| ${metric.replace(/_/g, ' ').split(' ').map(word => word.charAt(0).toUpperCase() + word.slice(1)).join(' ')} | ${value.toFixed(2)} |\n`;
                  }
                  
                  comment += '\n#### Document Statistics\n';
                  comment += '| Statistic | Value |\n';
                  comment += '|-----------|-------|\n';
                  for (const [stat, value] of Object.entries(result.statistics)) {
                    const formattedValue = stat.includes('time') ? 
                      `${value.toFixed(1)} minutes` : 
                      value.toFixed(2);
                    comment += `| ${stat.replace(/_/g, ' ').split(' ').map(word => word.charAt(0).toUpperCase() + word.slice(1)).join(' ')} | ${formattedValue} |\n`;
                  }
                  comment += '\n---\n\n';
                }
              } else {
                comment += '\n‚ö†Ô∏è No markdown files were analyzed in this PR.\n';
              }
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Error creating PR comment:', error);
            }
